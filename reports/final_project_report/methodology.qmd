##3 Methodology
#3.1 Introduction to Python for Machine Learning
The language of choice used for data processing, model creation, testing, training, and evaluation was Python. Python provides not only flexibility for the development of machine learning models but does so with a vast amount of open-source frameworks and libraries that are purpose-built for machine learning. The main two libraries of interest for this report are scikit-learn (sklearn) and pandas, both providing tools for data analysis, evaluation, and model building. Pandas was used for the packaging and manipulation of data through dataframes, while sklearn was used to provide the frameworks of the models and evaluation metrics (scikit-learn. (2019)). 

#3.2 Platform and Machine Configurations Used
The platforms used for development were a combination of Visual Studio Code (VSC), Jupyter Notebook, and GitHub. All development was done on local machines running Python3 and Jupyter to leverage the power of Jupyter notebooks in testing and development. VSC was largely used for Git version control, data pre-processing, and project organization, while Jupyter was used for the larger portion of the development of the models, testing, and evaluation for the Exploratory Data Analysis (EDA).

#3.3 Data Split
As mentioned above in section 2, the larger set of data was combined and randomly split into an 80:20 ratio of 1000 samples of data in a human-reviewed sample set. This was done to provide the perceived emotion of each string listed under the text and provide a ground truth for the testing and validation of the data. Although shrinking the dataset to only 1000 samples of training and testing data may provide less accuracy, it provides us with the ground truth of the data being predicted by the models and an easier correlation to the performance of the models used. As mentioned, after initial training and testing on the datasets, the models are then applied to make predictions on the original processed dataset for further evaluation without ground-truths. 

#3.4 Model Planning
After an investigation into which models may perform best for NLP of emotion prediction in text, the chosen models were a logistic regression model and a multinomial Naive Bayes model. Logistic Regression itself is a simpler supervised classification algorithm that makes it suitable for predicting categorical variables such as emotions (Kukunuri, R. (2018, December 5)). This type of model can be trained on the labels of emotions where each Tweet can be evaluated by the algorithm setting a list of weights across each to determine the highest probability of emotion in the text (Kukunuri, R. (2018, December 5)). Due to its relative efficiency and predictable outcomes, it made it an easy choice in the NLP performed in this investigation against multinomial Naive Bayes. 

The Multinomial Naive Bayes was chosen as the second model also due to its popularity in NLP, as it is more well suited for text classification where the features in the text (in this case; emotion) can be discrete of each other and provides a multinomial distribution of each label ((2021, January 3). UpGrad Blog). The algorithm itself calculates a prediction of each label by determining its correlation with the features discretely between each classification, however, is continuous between each classification to base the probability of emotion on the prior probability calculated (eiki. (2019, January 4)). This model also performs well using small training sets (Ratz, A. V. (2022, April 8)).

#3.5 Model Training
Training the multinomial Naive Bayes and Logistic Regression models was relatively straightforward as it consisted only of determining the emotion labels that were used in the human-sampled dataset and the text features associated with each human-reviewed label for the text in each Tweet. 

For the Logistic Regression model, the X and y values were set to the contents of the column “Tweet” holding the list of tweets, and “Ground Truth” holding the list of labels from the human reviewed portions of the data. After the data was split the model was made from a pipeline with steps CountVectorizer and LogisticRegression.

For the multinomial Naive Bayes model, the labels were determined by the “Ground Truth” column in the sample dataset to determine the emotion labels for classification. The features of the text were defined using a CountVectorizer, a process of converting a collection of words in a string into a matrix of token counts, which can be used and analyzed by the model for training the fitness ((2018). Scikit-Learn.org).

Due to the simplicity of feature extraction and training of each model, no adjustment to the model parameters, labels, or features was required to be made for their corresponding performance metrics.   

#3.6 Model Evaluation
In order to evaluate the Logistic Regression model, it was decided that a manual approach would give the best yield. In order to accomplish this, a new column was created called “Predictions”. This column was filled with the values predicted by the model. To complement this column, a second was created, this column was called the “Match” column. Within the “Match” column, the predictions were compared against the ground truth for each tweet and a boolean value of “True” or “False” was assigned. This column was then used in all evaluation metrics.

After a first-glance evaluation of the results of the multinomial Naive Bayes model, it determined a fitness score of only 44%. However, on evaluation of correct predictions against a few test vectors as well as the ground truth of the combined training and testing sample sets (predictions and matches columns), it was determined that the model is closer to 76% accurate in making correct decisions on the sample text. The overall lack of performance produced by this model likely has a relation to the rather small sample set and distribution of emotions across the dataset (i.e., 271 happiness versus 15 fear labels). Therefore, when the model makes predictions based on the text, it may have had a tendency to lean more towards one emotion more than another due to its behavior to determine new classifications based on already calculated data. Additionally, this performance may have been impacted by the rather obscure word choice of many Tweets within the dataset and in general, as Tweets generally use “looser” terminology and abbreviations of sentences (such as “lol”). 

#3.7 Model Optimization
Based on the results of the model evaluations, optimization was then done to improve the performance of the models. Optimization consisted of training the models on different datasets to see how they would perform. Datasets containing emotional analysis of text were used in the hopes of fine tuning our model on the emotion already considered and one additional emotion. This approach improved the Multinomial Naive Bayes model and hindered the Logistic Regression model. Another thing to consider was the fact that the models were trained on a sample. Each model likely would have improved if the human reviewed sampling set was larger than 1000, however, due to time constraints this was not feasible during the investigation. Multiple training and test ratios were also considered before finally settling on the best performing 80:20 split.

#3.8 Final Model Building
After optimizing the models, the final step was to fit the models and report the final test errors per model. Below are a series of screenshots showing some of the calculations made after the models were fit to measure error.
